{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T21:32:15.919140Z",
     "start_time": "2024-02-21T21:32:14.783025Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T21:32:15.923818Z",
     "start_time": "2024-02-21T21:32:15.922291Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l=[2, 3, 2]):\n",
    "        super().__init__()\n",
    "\n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l) - 1\n",
    "        self.n_l = n_l\n",
    "\n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i: None for i in range(1, self.L + 1)}\n",
    "        self.a = {i: None for i in range(self.L + 1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i: None for i in range(1, self.L + 1)}\n",
    "        self.dL_db = {i: None for i in range(1, self.L + 1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i: lambda x: torch.tanh(x) for i in range(1, self.L + 1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i: lambda x: (1 / (torch.cosh(x) ** 2))\n",
    "            for i in range(1, self.L + 1)\n",
    "        }\n",
    "\n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L + 1)})\n",
    "        for i in range(1, self.L + 1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i - 1], out_features=n_l[i])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "\n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L + 1):\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i - 1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T21:32:15.927140Z",
     "start_time": "2024-02-21T21:32:15.925035Z"
    }
   },
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    with torch.no_grad():\n",
    "        # Calculate the gradient of the loss with respect to the predictions\n",
    "        # For MSE, this is simply the difference between predictions and true values\n",
    "        dL_dy = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Iterate over layers in reverse order to perform backpropagation\n",
    "        for l in range(model.L, 0, -1):\n",
    "            # Calculate the gradient of the loss with respect to z (pre-activation)\n",
    "            # This involves the derivative of the activation function\n",
    "            dL_dz = model.df[l](model.z[l]) * dL_dy\n",
    "\n",
    "            a_prev_layer = model.a[l - 1]\n",
    "\n",
    "            # Calculate the gradient of the loss with respect to the weights and biases\n",
    "            # For weights: it's the outer product of dL_dz and activations of the previous layer\n",
    "            model.dL_dw[l] = torch.mm(dL_dz.t(), a_prev_layer)\n",
    "\n",
    "            # For biases: since dL_dz is already a 2D tensor, sum over its only dimension for the batch\n",
    "            model.dL_db[l] = dL_dz.sum(dim=0)\n",
    "\n",
    "            # Update dL_dy for the next layer (propagate the gradient through the network)\n",
    "            # This requires computing the gradient of the loss with respect to the activations of the previous layer\n",
    "            if l > 1:  # No need to compute if l == 1 since there's no previous layer to propagate to\n",
    "                dL_dy = torch.mm(dL_dz, model.fc[str(l)].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T21:32:16.151653Z",
     "start_time": "2024-02-21T21:32:15.927835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=False, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[  1.7421e-27,   1.7421e-27,   1.7421e-27,  ...,   1.7421e-27,\n",
      "           1.7421e-27,   1.7421e-27],\n",
      "        [-5.9696e-177, -5.9696e-177, -5.9696e-177,  ..., -5.9696e-177,\n",
      "         -5.9696e-177, -5.9696e-177],\n",
      "        [ -3.7006e-19,  -3.7006e-19,  -3.7006e-19,  ...,  -3.7006e-19,\n",
      "          -3.7006e-19,  -3.7006e-19],\n",
      "        ...,\n",
      "        [-1.6763e-132, -1.6763e-132, -1.6763e-132,  ..., -1.6763e-132,\n",
      "         -1.6763e-132, -1.6763e-132],\n",
      "        [  1.4466e-40,   1.4466e-40,   1.4466e-40,  ...,   1.4466e-40,\n",
      "           1.4466e-40,   1.4466e-40],\n",
      "        [  9.5900e-56,   9.5900e-56,   9.5900e-56,  ...,   9.5900e-56,\n",
      "           9.5900e-56,   9.5900e-56]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-4.1084e-27, 1.4078e-176,  8.7273e-19, -2.0038e+01,  2.7808e-77,\n",
      "         6.5821e-18, -3.4859e-43, -7.9225e-37, -3.4193e-25,  4.3084e-25,\n",
      "        -1.1987e-24, -2.1216e-34, -2.4377e-30, 3.9534e-132, -3.4116e-40,\n",
      "        -2.2617e-55])\n",
      "  Autograd's computation:\n",
      " tensor([  0.0000,   0.0000,   0.0000, -20.0377,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.2101, -0.2493, -0.1720,  ..., -0.1747, -0.2023, -0.2544],\n",
      "        [ 0.6405,  0.6012,  0.6404,  ...,  0.6005,  0.6012,  0.6509],\n",
      "        [ 0.2595,  0.2065,  0.2728,  ...,  0.2550,  0.2008,  0.2776],\n",
      "        ...,\n",
      "        [ 0.4555,  0.4385,  0.5049,  ...,  0.4893,  0.4710,  0.4452],\n",
      "        [ 0.0960,  0.1214,  0.0961,  ...,  0.0811,  0.1427,  0.1286],\n",
      "        [-1.0650, -1.0229, -1.0307,  ..., -1.0668, -1.0503, -1.0247]])\n",
      "tensor([ 0.4872, -1.4792, -0.5399,  2.7849, -0.6366, -1.5495, -0.3878,  0.3257,\n",
      "         0.8094, -1.5389,  1.3450,  0.2497,  0.1751, -1.1013, -0.2663,  2.4902])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24 * 24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=True, data='mnist')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:32:31.586097Z",
     "start_time": "2024-02-21T21:32:16.151578Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T21:32:46.727774Z",
     "start_time": "2024-02-21T21:32:31.584646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24 * 24, 16, 1])  # 1-dimensional output layer, without warning\n",
    "main_test(backpropagation, model, verbose=False, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
