{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.158994Z",
     "start_time": "2024-04-16T21:20:49.182705Z"
    }
   },
   "id": "dc3749b76e893bd4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "torch.manual_seed(256)\n",
    "torch.cuda.manual_seed(256)\n",
    "np.random.seed(256)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.164134Z",
     "start_time": "2024-04-16T21:20:50.160243Z"
    }
   },
   "id": "7b216aac8fa49f71"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.165275Z",
     "start_time": "2024-04-16T21:20:50.163244Z"
    }
   },
   "id": "8ebf58a160f821f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Word embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56f18624e811dc2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Read txt files and tokenize them to obtain train/validation/test lists of words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecdeb62e62883d05"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def read_txt_files(datapath):\n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith(\".txt\")]\n",
    "\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        with open(f_name) as f:\n",
    "            lines += f.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def tokenize(lines, tokenizer=TOKENIZER):\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text\n",
    "\n",
    "\n",
    "def yield_tokens(lines, tokenizer=TOKENIZER):\n",
    "    no_digits = \"\\w*[0-9]+\\w*\"  # Regex to match words containing numbers\n",
    "    no_names = \"\\w*[A-Z]+\\w*\"  # Regex to match words with capital letters (names)\n",
    "    no_spaces = \"\\s+\"  # Regex to match sequences of whitespace\n",
    "\n",
    "    # Processing each line to remove digits, names, and extra spaces\n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits, \" \", line)\n",
    "        line = re.sub(no_names, \" \", line)\n",
    "        line = re.sub(no_spaces, \" \", line)\n",
    "        # Yielding the tokenized and cleaned line\n",
    "        yield tokenizer(line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.171816Z",
     "start_time": "2024-04-16T21:20:50.167134Z"
    }
   },
   "id": "180dd603429352f4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "GENERATED_PATH = \"./generated/\"  # Path where generated data files are stored\n",
    "\n",
    "# Check if the training data file already exists in the generated path\n",
    "if os.path.isfile(GENERATED_PATH + \"words_train.pt\"):\n",
    "    # Load preprocessed training, validation, and test word lists from .pt files\n",
    "    words_train = torch.load(GENERATED_PATH + \"words_train.pt\")\n",
    "    words_val = torch.load(GENERATED_PATH + \"words_val.pt\")\n",
    "    words_test = torch.load(GENERATED_PATH + \"words_test.pt\")\n",
    "else:\n",
    "    # If preprocessed data does not exist, read text files\n",
    "    lines_books_train = read_txt_files(\"data/data_train/\")\n",
    "    lines_books_val = read_txt_files(\"data/data_val/\")\n",
    "    lines_books_test = read_txt_files(\"data/data_test/\")\n",
    "\n",
    "    # Tokenize the lines from train, validation, and test datasets\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val = tokenize(lines_books_val)\n",
    "    words_test = tokenize(lines_books_test)\n",
    "\n",
    "    # Save the tokenized word lists to .pt files\n",
    "    torch.save(words_train, GENERATED_PATH + \"words_train.pt\")\n",
    "    torch.save(words_val, GENERATED_PATH + \"words_val.pt\")\n",
    "    torch.save(words_test, GENERATED_PATH + \"words_test.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.545263Z",
     "start_time": "2024-04-16T21:20:50.170846Z"
    }
   },
   "id": "e46a7ce8b9c09f97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Define a vocabulary based on the training dataset. To avoid getting a too large vocabulary, a solution can be to keep only words that appear at least 100 times in the training dataset. Report the total number of words in the training dataset, the number of distinct words in the training dataset, and the size of the defined vocabulary. Comment on your results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3d6615cee6d135c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "MIN_FREQ = 100\n",
    "\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    # Building vocabulary from an iterator of tokenized lines, filtering out infrequent tokens\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    # Appending token \"I\", since we removed all words with an uppercase when building the vocabulary\n",
    "    vocab.append_token(\"i\")\n",
    "    # Setting default index for unknown words\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.553249Z",
     "start_time": "2024-04-16T21:20:50.545796Z"
    }
   },
   "id": "79cd1cab7154c135"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "VOCAB_FILENAME = \"vocabulary.pt\"\n",
    "\n",
    "# Check if the vocabulary file already exists in the generated path\n",
    "if os.path.isfile(GENERATED_PATH + VOCAB_FILENAME):\n",
    "    # Load the vocabulary from a file if it already exists\n",
    "    vocab = torch.load(GENERATED_PATH + VOCAB_FILENAME)\n",
    "else:\n",
    "    # If the vocabulary file does not exist, create a new vocabulary from training data\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    # Save the newly created vocabulary to a file\n",
    "    torch.save(vocab, GENERATED_PATH + VOCAB_FILENAME)\n",
    "\n",
    "VOCAB_SIZE = len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.562565Z",
     "start_time": "2024-04-16T21:20:50.548677Z"
    }
   },
   "id": "cf7baee7fcace153"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset: 2,684,706\n",
      "Total number of words in the validation dataset: 49,526\n",
      "Total number of words in the test dataset: 124,152\n",
      "\n",
      "Number of distinct words in the training dataset: 52,105\n",
      "Number of distinct words in the validation dataset: 5,778\n",
      "Number of distinct words in the test dataset: 9,585\n",
      "\n",
      "Size of the defined vocabulary: 1,880\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of words in the training dataset: {len(words_train):,}\")\n",
    "print(f\"Total number of words in the validation dataset: {len(words_val):,}\")\n",
    "print(f\"Total number of words in the test dataset: {len(words_test):,}\", end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Number of distinct words in the training dataset: {len(set(words_train)):,}\")\n",
    "print(f\"Number of distinct words in the validation dataset: {len(set(words_val)):,}\")\n",
    "print(f\"Number of distinct words in the test dataset: {len(set(words_test)):,}\", end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Size of the defined vocabulary: {VOCAB_SIZE:,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.655229Z",
     "start_time": "2024-04-16T21:20:50.653454Z"
    }
   },
   "id": "9f174be539d0b11b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def count_occurrences(words, vocab):\n",
    "    occurrences = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        occurrences[vocab[w]] += 1\n",
    "    return occurrences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:50.660432Z",
     "start_time": "2024-04-16T21:20:50.655942Z"
    }
   },
   "id": "b4c4165ae11f6e10"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "word_counts_df = pd.DataFrame({\n",
    "    \"Word\": vocab.lookup_tokens(range(len(vocab))),\n",
    "    \"Occurrences\": count_occurrences(words_train, vocab).numpy()\n",
    "})\n",
    "\n",
    "sorted_word_counts = word_counts_df.sort_values(by=\"Occurrences\", ascending=False).reset_index(drop=True)\n",
    "sorted_word_counts.index = sorted_word_counts.index + 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:58.988124Z",
     "start_time": "2024-04-16T21:20:50.658863Z"
    }
   },
   "id": "1e516a890cd3ce97"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                Word  Occurrences\n1              <unk>       433907\n2                  ,       182537\n3                the       151278\n4                  .       123727\n5                and        82289\n...              ...          ...\n1876          pistol          100\n1877         slipped          100\n1878  station-master          100\n1879          wounds          100\n1880           agree          100\n\n[1880 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Occurrences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>&lt;unk&gt;</td>\n      <td>433907</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>,</td>\n      <td>182537</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the</td>\n      <td>151278</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>.</td>\n      <td>123727</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>and</td>\n      <td>82289</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1876</th>\n      <td>pistol</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1877</th>\n      <td>slipped</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1878</th>\n      <td>station-master</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1879</th>\n      <td>wounds</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1880</th>\n      <td>agree</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n<p>1880 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:59.006532Z",
     "start_time": "2024-04-16T21:20:58.998188Z"
    }
   },
   "id": "2aa1dbc37499968d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_word_type_label(word):\n",
    "    if word == '<unk>':  # Unknown word\n",
    "        return 0\n",
    "    elif word in [',', '.', '(', ')', '?', '!']:  # Punctuation\n",
    "        return 1\n",
    "    else:  # A valid word that exists in the vocabulary\n",
    "        return 2\n",
    "\n",
    "\n",
    "# Map each vocabulary word to its appropriate type label\n",
    "MAP_TARGET = {vocab[w]: get_word_type_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))}\n",
    "CONTEXT_SIZE = 3  # Number of words considered before the target word\n",
    "\n",
    "\n",
    "def create_context_target_dataset(text, vocab, context_size=CONTEXT_SIZE, map_target=MAP_TARGET):\n",
    "    n_text = len(text)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(n_text - context_size):\n",
    "        # Extract the context words and convert them to indices using the vocabulary\n",
    "        context = [vocab[word] for word in text[i:i + context_size]]\n",
    "        # Get the target word and convert it to its label using the map_target dictionary\n",
    "        target = map_target[vocab[text[i + context_size]]]\n",
    "        contexts.append(torch.tensor(context))\n",
    "        targets.append(target)\n",
    "\n",
    "    return TensorDataset(torch.stack(contexts), torch.tensor(targets))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:59.011530Z",
     "start_time": "2024-04-16T21:20:59.005608Z"
    }
   },
   "id": "1121f0251a057029"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, filename, generated_path=GENERATED_PATH):\n",
    "    full_path = os.path.join(generated_path, filename)\n",
    "    if os.path.isfile(full_path):\n",
    "        return torch.load(full_path)\n",
    "    else:\n",
    "        dataset = create_context_target_dataset(words, vocab)\n",
    "        torch.save(dataset, full_path)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\")\n",
    "data_val = load_dataset(words_val, vocab, \"data_val.pt\")\n",
    "data_test = load_dataset(words_test, vocab, \"data_test.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:59.041046Z",
     "start_time": "2024-04-16T21:20:59.008261Z"
    }
   },
   "id": "ec1c029e087bb33a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 2,684,703\n",
      "Validation dataset size: 49,523\n",
      "Test dataset size: 124,149\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset size: {len(data_train):,}\")\n",
    "print(f\"Validation dataset size: {len(data_val):,}\")\n",
    "print(f\"Test dataset size: {len(data_test):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:59.041551Z",
     "start_time": "2024-04-16T21:20:59.036293Z"
    }
   },
   "id": "4f5b00a12bc9f5d8"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:59.041656Z",
     "start_time": "2024-04-16T21:20:59.038735Z"
    }
   },
   "id": "936f6b3b186dcabe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Define a continuous bag of words model architecture based on this vocabulary that contains an embedding layer. To drastically reduce the computational cost, the dimension of the embedding `emb_dim` can be very low such as 16, 12, or even 10. Of course, in a real setting, a larger space would be used. You are not allowed to use `nn.LazyLinear` in this project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e36f612c053685"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.fc1 = nn.Linear(emb_dim * context_size, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:59.053563Z",
     "start_time": "2024-04-16T21:20:59.042026Z"
    }
   },
   "id": "b23551ec33fe2857"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "CBOW_model = CBOW(vocab_size=VOCAB_SIZE, emb_dim=12, context_size=CONTEXT_SIZE).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  # softmax + negative log-likelihood \n",
    "optimizer = optim.Adam(CBOW_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=20):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for contexts, targets in train_loader:\n",
    "            contexts, targets = contexts.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "\n",
    "        timestamp = datetime.now().strftime('%H:%M:%S.%f')\n",
    "        print(f\"{timestamp} | Epoch {epoch} | Training Loss: {avg_loss:.5f} | \", end=\"\")\n",
    "        evaluate_model(model, val_loader, loss_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:59.496581Z",
     "start_time": "2024-04-16T21:20:59.045329Z"
    }
   },
   "id": "90cbb60d8c0c5343"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts, targets = contexts.to(device), targets.to(device)\n",
    "            outputs = model(contexts)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_loss / len(loader):.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:20:59.496729Z",
     "start_time": "2024-04-16T21:20:59.494596Z"
    }
   },
   "id": "f137bef256bb524d"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:22:50.901142 | Epoch 1 | Training Loss: 0.68733 | Validation Loss: 0.62065\n",
      "23:24:38.661957 | Epoch 2 | Training Loss: 0.64490 | Validation Loss: 0.61411\n",
      "23:28:07.504081 | Epoch 3 | Training Loss: 0.63909 | Validation Loss: 0.61014\n",
      "23:29:53.631149 | Epoch 4 | Training Loss: 0.63563 | Validation Loss: 0.61247\n",
      "23:31:47.059435 | Epoch 5 | Training Loss: 0.63322 | Validation Loss: 0.60702\n"
     ]
    }
   ],
   "source": [
    "train_model(CBOW_model, train_loader, val_loader, optimizer, loss_fn, epochs=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:31:47.774851Z",
     "start_time": "2024-04-16T21:20:59.496687Z"
    }
   },
   "id": "3d929d2c9ef2eb7b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:31:47.776469Z",
     "start_time": "2024-04-16T21:31:47.771690Z"
    }
   },
   "id": "532816665732979"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
