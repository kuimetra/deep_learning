{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torchvision"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfb7700abf36813e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 256\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a6590332964997"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Load, analyse and preprocess the CIFAR-10 dataset. Split it into 3 datasets: training, validation and test.\n",
    "Take a subset of these datasets by keeping only 2 labels: bird and plane."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22d1a00ec8a9ffa0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    return cifar10_train, cifar10_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b223c78d78e36976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cifar10_train, cifar10_test = load_cifar10()\n",
    "label_map = {0: 0, 2: 1}\n",
    "\n",
    "cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in [0, 2]]\n",
    "cifar2_test = [(img, label_map[label]) for img, label in cifar10_test if label in [0, 2]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1e2c1b0540db502"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_total = len(cifar2_train)\n",
    "num_train = int(0.8 * num_train_total)\n",
    "num_val = num_train_total - num_train\n",
    "\n",
    "cifar2_train, cifar2_val = random_split(cifar2_train, [num_train, num_val])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e215fe4a2d718f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Train Size:\", len(cifar2_train))\n",
    "print(\"Validation Size:\", len(cifar2_val))\n",
    "print(\"Test Size:\", len(cifar2_test))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a41cd52127afb84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, dataset in zip(['Train', 'Validation', 'Test'], [cifar2_train, cifar2_val, cifar2_test]):\n",
    "    counts = [sum(label == i for _, label in dataset) for i in range(2)]\n",
    "    print(f\"{name}: {counts[0]} Airplanes, {counts[1]} Birds\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b3fd79cb9952e67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_imgs(imgs, titles, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 3))\n",
    "\n",
    "    for i, (img, title) in enumerate(zip(imgs, titles)):\n",
    "        img = img / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        axes[row, col].set_title(title)\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "airplane_indices = [i for i, (img, label) in enumerate(cifar2_train) if label == 0][:8]\n",
    "bird_indices = [i for i, (img, label) in enumerate(cifar2_train) if label == 1][:8]\n",
    "\n",
    "airplane_images = [cifar2_train[i][0] for i in airplane_indices]\n",
    "bird_images = [cifar2_train[i][0] for i in bird_indices]\n",
    "\n",
    "show_imgs(airplane_images + bird_images, ['Airplane'] * 8 + ['Bird'] * 8, rows=2, cols=8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e44da8c241937d43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on device {device}.\")\n",
    "    return device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffa50a32d998f84c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Write a `MyMLP` class that implements a MLP in PyTorch (so only fully connected layers) such that:\n",
    "(a) The input dimension is 3072 (= 32 $\\times$ 32 $\\times$ 3) and the output dimension is 2 (for the 2 classes).\n",
    "(b) The hidden layers have respectively 512, 128 and 32 hidden units.\n",
    "(c) All activation functions are `ReLU`. The last layer has no activation function since the\n",
    "cross-entropy loss already includes a softmax activation function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1524fc332af75c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # transforming into 1-dim vector \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26bc3ea2a234bd44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Write a `train(n_epochs, optimizer, model, loss_fn, train_loader)` function that trains model for `n_epochs`\n",
    "epochs given an optimizer `optimizer`, a loss function `loss_fn` and a dataloader `train_loader`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcf3f02347e6f14b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    print(\" --------- Using Pytorch's SGD ---------\")\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device=device, dtype=torch.double), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            timestamp = datetime.now().strftime('%H:%M:%S.%f')\n",
    "            avg_loss = train_loss / len(train_loader)\n",
    "            print(f\"{timestamp}  |  Epoch {epoch}  |  Training loss {round(avg_loss, 5)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c411bac35f2eeee3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Write a similar function `train_manual_update` that has no `optimizer` parameter, but a learning rate `lr` parameter instead and that manually updates each trainable parameter of `model` using equation (2). Do not forget to zero out all gradients after each iteration.\n",
    "6. Modify `train_manual_update` by adding a L2 regularization term in your manual parameter update. Add an additional weight decay parameter to `train_manual_update`. Compare again `train` and `train_manual_update` results with 0 < `weight_decay` < 1.\n",
    "7. Modify `train_manual_update` by adding a momentum term in your parameter update. Add an additional `momentum` parameter to `train_manual_update`. Check again the correctness of the new update rule by comparing it to train function (with 0 < `momentum` < 1).\n",
    "\n",
    "\n",
    "[SGD PyTorch Pseudocode](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67a5df82b4cac140"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, model, loss_fn, train_loader, weight_decay, momentum):\n",
    "    print(\" --------- Using manual update ----------\")\n",
    "\n",
    "    momentum_buffers = {name: torch.zeros_like(p.data) for name, p in model.named_parameters()}\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device=device, dtype=torch.double), labels.to(device)\n",
    "\n",
    "            # forward + backward\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # manual parameter update using the gradient descent rule\n",
    "            with torch.no_grad():\n",
    "                for name, p in model.named_parameters():\n",
    "                    grad = p.grad\n",
    "\n",
    "                    # L2 regularization: adjust the gradient by adding weight_decay (L2 penalty) \n",
    "                    # times the parameters from the previous time step\n",
    "                    grad += weight_decay * p.data\n",
    "\n",
    "                    # compute the new buffer (accumulation of the gradient over time, scaled by \n",
    "                    # the momentum) using the previous buffer and the current gradient\n",
    "                    buf = momentum_buffers[name]\n",
    "                    buf.mul_(momentum).add_(grad)\n",
    "                    grad = buf\n",
    "\n",
    "                    # update the parameters, using the adjusted gradient and the learning rate lr\n",
    "                    p.data -= lr * grad\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            timestamp = datetime.now().strftime('%H:%M:%S.%f')\n",
    "            avg_loss = train_loss / len(train_loader)\n",
    "            print(f\"{timestamp}  |  Epoch {epoch}  |  Training loss {round(avg_loss, 5)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bdb0a49dce26e74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, loader):\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device=device, dtype=torch.double), labels.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            # class with the highest value is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aab13bbda983fb9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(cifar2_train, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(cifar2_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(cifar2_test, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efdccb56a455a368"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "l_rate = 0.01\n",
    "momentum = [0, 0, 0.9, 0.9, 0.9, 0.8]\n",
    "weight_decay = [0, 0.01, 0, 0.01, 0.001, 0.01]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3575d533477a42b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = set_device()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f24dc9d5e2662362"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for mom, decay in zip(momentum, weight_decay):\n",
    "    print(\" =========================================================\")\n",
    "    print(f\"   Current parameters:\\nlr = {l_rate}\\nmom = {mom}\\ndecay = {decay}\\n\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model1 = MyMLP().to(device)\n",
    "    optimizer = optim.SGD(model1.parameters(), lr=l_rate, momentum=mom, weight_decay=decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train(n_epochs, optimizer, model1, loss_fn, train_loader)\n",
    "\n",
    "    train_accuracy_model1 = evaluate_accuracy(model1, train_loader)\n",
    "    val_accuracy_model1 = evaluate_accuracy(model1, val_loader)\n",
    "    print(\"\\n --- Accuracies ---\")\n",
    "    print(f\"Training\\nAccuracy: {round(train_accuracy_model1, 2)}\")\n",
    "    print(f\"Validation\\nAccuracy: {round(val_accuracy_model1, 2)}\\n\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model2 = MyMLP().to(device)\n",
    "\n",
    "    train_manual_update(n_epochs, l_rate, model2, loss_fn, train_loader, decay, mom)\n",
    "\n",
    "    train_accuracy_model2 = evaluate_accuracy(model2, train_loader)\n",
    "    val_accuracy_model2 = evaluate_accuracy(model2, val_loader)\n",
    "    print(\"\\n --- Accuracies ---\")\n",
    "    print(f\"Training\\nAccuracy: {round(train_accuracy_model2, 2)}\")\n",
    "    print(f\"Validation\\nAccuracy: {round(val_accuracy_model2, 2)}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c4bec2a5ad38fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "892205591a87c95f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
